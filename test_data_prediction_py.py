# -*- coding: utf-8 -*-
"""test_data_prediction.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-dvn2fmW61MNnMZe-c49KDI3a2s2QIK6
"""

import os
import numpy as np
import librosa
import joblib
from sklearn.preprocessing import LabelEncoder

model = joblib.load("/content/drive/MyDrive/emotion_classification/trained_model.pkl")
label_encoder = joblib.load("/content/drive/MyDrive/emotion_classification/label_encoder.pkl")

def extract_audio_features(file_path, sr=22050, n_mels=128, duration=3, hop_length=512):
    try:
        y, _ = librosa.load(file_path, sr=sr, duration=duration)
        if len(y) < sr * duration:
            y = np.pad(y, (0, sr * duration - len(y)))

        # Mel Spectrogram (mean)
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)
        mel_mean = np.mean(librosa.power_to_db(mel_spec, ref=np.max), axis=1)

        # MFCCs and Deltas
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        delta = librosa.feature.delta(mfccs)
        delta2 = librosa.feature.delta(mfccs, order=2)

        mfcc_mean = np.mean(mfccs.T, axis=0)
        delta_mean = np.mean(delta.T, axis=0)
        delta2_mean = np.mean(delta2.T, axis=0)

        # Chroma, Contrast, Tonnetz
        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)
        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0)

        # ZCR, RMS, Centroid, Rolloff
        zcr = np.mean(librosa.feature.zero_crossing_rate(y))
        rmse = np.mean(librosa.feature.rms(y=y))

        # Dummy gender & intensity if unknown (can be inferred if RAVDESS style)
        gender = 1  # Placeholder: 1 = male, 0 = female
        intensity = 2  # Placeholder: 1 = normal, 2 = strong

        features = np.hstack([
            mel_mean,
            mfcc_mean,
            delta_mean,
            delta2_mean,
            chroma,
            contrast,
            tonnetz,
            zcr,
            rmse,
            gender,
            intensity
        ])
        return features

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def predict_emotion(file_path):
    features = extract_audio_features(file_path)
    if features is not None:
        features = features.reshape(1, -1)
        prediction = model.predict(features)
        # Convert prediction to integer type before inverse transforming
        predicted_label = label_encoder.inverse_transform(prediction.astype(int).flatten())
        print(f"\n File: {os.path.basename(file_path)}")
        print(f" Predicted Emotion: {predicted_label[0]}")
    else:
        print(" Feature extraction failed.")

if __name__ == "__main__":
    # Replace with path to your .wav test file
    test_audio_file = "give your file path"
    predict_emotion(test_audio_file)

