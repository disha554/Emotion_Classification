# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aslg9Rlx9fhAHSz-0YLkf5Xo6CD3BP4W
"""



import streamlit as st
import numpy as np
import librosa
import joblib
import os

# Load trained model and label encoder
@st.cache_resource
def load_model():    
    model = joblib.load("trained_model.pkl")
    label_encoder = joblib.load("label_encoder.pkl")
    return model, label_encoder
    
    
       
model, label_encoder = load_model()

# Feature extraction function
def extract_audio_features(file_path, sr=22050, n_mels=128, duration=3, hop_length=512):
    try:
        y, _ = librosa.load(file_path, sr=sr, duration=duration)
        if len(y) < sr * duration:
            y = np.pad(y, (0, sr * duration - len(y)))

        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)
        mel_mean = np.mean(librosa.power_to_db(mel_spec, ref=np.max), axis=1)

        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        delta = librosa.feature.delta(mfccs)
        delta2 = librosa.feature.delta(mfccs, order=2)

        mfcc_mean = np.mean(mfccs.T, axis=0)
        delta_mean = np.mean(delta.T, axis=0)
        delta2_mean = np.mean(delta2.T, axis=0)

        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)
        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0)

        zcr = np.mean(librosa.feature.zero_crossing_rate(y))
        rmse = np.mean(librosa.feature.rms(y=y))
        centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
        rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))

        gender = 1  # Placeholder (you can change based on filename if needed)
        intensity = 2  # Placeholder

        features = np.hstack([
            mel_mean,
            mfcc_mean,
            delta_mean,
            delta2_mean,
            chroma,
            contrast,
            tonnetz,
            zcr,
            rmse,
            gender,
            intensity
        ])
        return features

    except Exception as e:
        st.error(f"Feature extraction error: {e}")
        return None

# Streamlit UI
st.title("Emotion Detection from Voice")
st.write("Upload a `.wav` audio file to predict the emotion it conveys.")

uploaded_file = st.file_uploader("Choose a .wav file", type=["wav"])

if uploaded_file is not None:
    with open("temp_audio.wav", "wb") as f:
        f.write(uploaded_file.read())

    st.audio(uploaded_file, format='audio/wav')

    features = extract_audio_features("temp_audio.wav")

    if features is not None:
        features = features.reshape(1, -1)
        try:
            prediction = model.predict(features)
            emotion = label_encoder.inverse_transform(prediction)[0]
            st.success(f" Predicted Emotion: **{emotion}**")
        except Exception as e:
            st.warning(" Prediction failed: Emotion class not recognized or out of training scope.")
            st.text(f"Details: {e}")





